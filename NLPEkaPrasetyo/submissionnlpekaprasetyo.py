# -*- coding: utf-8 -*-
"""SubmissionNLPEkaPrasetyo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dPptH0gDH1U3N3xyPo_kMgK37mcuY_qg
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/vgsales.csv', sep=',') 
df = df.drop(columns=['Publisher'])

df.head()

df.tail()

df.isna().sum()

df.dropna(inplace=True)

df = df[df['Type'] != 'PS2']

df['Publisher'] = np.where(df['Type'] > 'PS2',1,0)
df.head(10)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df['Name'], 
                                                    df['Genre'], 
                                                    random_state=0)

print('X_train first entry:\n\n', X_train.iloc[0])
print('\n\nX_train shape: ', X_train.shape)

from sklearn.feature_extraction.text import CountVectorizer


vect = CountVectorizer().fit(X_train)

vect.get_feature_names()[::2000]

len(vect.get_feature_names())

X_train_vectorized = vect.transform(X_train)

X_train_vectorized

from sklearn.linear_model import LogisticRegression


model = LogisticRegression()
model.fit(X_train_vectorized, y_train)

feature_names = np.array(vect.get_feature_names())
 
 
sorted_coef_index = model.coef_[0].argsort()
 
 
print('koefisien terkecil:\n{}\n'.format(feature_names[sorted_coef_index[:10]]))
print('koefisien terbesar: \n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))

from sklearn.feature_extraction.text import TfidfVectorizer
 
# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5
vect = TfidfVectorizer(min_df=1).fit(X_train)
len(vect.get_feature_names())

feature_names = np.array(vect.get_feature_names())
 
sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()
 
print('tfidf terkecil:\n{}\n'.format(feature_names[sorted_tfidf_index[:10]]))
print('tfidf terbesar: \n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))

sorted_coef_index = model.coef_[0].argsort()
 
print('koefisien terkecil:\n{}\n'.format(feature_names[sorted_coef_index[:10]]))
print('koefisien terbesar: \n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))

print(model.predict(vect.transform(['autosport is european wars',
                                    'skylander is not european wars'])))

vect = CountVectorizer(min_df=2, ngram_range=(1,3)).fit(X_train)
 
X_train_vectorized = vect.transform(X_train)
 
len(vect.get_feature_names())

feature_names = np.array(vect.get_feature_names())
 
sorted_coef_index = model.coef_[0].argsort()
 
print('koefisien terkecil:\n{}\n'.format(feature_names[sorted_coef_index[:10]]))
print('koefisien terbesar: \n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))

df.groupby('Genre').count()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
 
# %matplotlib inline
 
df.groupby('Genre').Genre.count().plot.bar()
plt.title("Video Games Categories Distribution")
plt.xlabel('Genre')
plt.ylabel('Counts')
plt.show()

df[df['Genre'] == 'Action'].head(10)

df.info()

category = pd.get_dummies(df.Genre)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='Genre')
df_baru

dataset = df_baru.values
dataset

Nama = df_baru['Name'].values
genre = df_baru[['Action','Adventure','Fighting','Misc','Platform','Puzzle', 'Racing', 'Role-Playing', 'Shooter', 'Simulation','Sports','Strategy']].values

print(df_baru[['Action','Adventure','Fighting','Misc','Platform','Puzzle', 'Racing', 'Role-Playing', 'Shooter', 'Simulation','Sports','Strategy']])

from sklearn.model_selection import train_test_split
Nama_latih, Nama_test, genre_latih, genre_test = train_test_split(Nama, genre, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, oov_token='-')
tokenizer.fit_on_texts(Nama_latih) 
tokenizer.fit_on_texts(Nama_test)
 
sekuens_latih = tokenizer.texts_to_sequences(Nama_latih)
sekuens_test = tokenizer.texts_to_sequences(Nama_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

print(Nama_latih[2])
print(sekuens_latih[2])

print(tokenizer.word_index)

for word in ['the', 'all', 'happy', 'world','game','super']:
    print('{}: {}'.format(word, tokenizer.word_index[word]))

print(sekuens_latih)

print(sekuens_test)

print(padded_latih)

print(padded_test)

# Contoh Vektor
contoh  =[Nama[0],Nama[1],Nama[2],Nama[3]] # Wii Sports [0],Super Mario Bros.[1],Mario Kart Wii[3],Wii Sports Resort[4]

from sklearn.feature_extraction.text import CountVectorizer # CountVectorizer yang disediakan oleh scikit-learn library untuk membuat vektor kalimat.
vectorizer = CountVectorizer(min_df=0, lowercase=False)
vectorizer.fit(contoh)
vectorizer.vocabulary_

vectorizer.transform(contoh).toarray()

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=50),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(12, activation='softmax')
])

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

num_epochs = 50
history = model.fit(padded_latih, genre_latih, epochs=num_epochs, batch_size=128, validation_data=(padded_test, genre_test),verbose=2, callbacks=[callbacks])

model.evaluate(padded_latih, genre_latih)

model.evaluate(padded_test, genre_test)

model.predict(padded_latih[1])

model.predict(padded_test[1])

model.predict(genre_latih[1])

model.predict(genre_test[1])

# Visualize training history
from keras.models import Sequential
from keras.layers import Dense
import matplotlib.pyplot as plt
import numpy
 
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()